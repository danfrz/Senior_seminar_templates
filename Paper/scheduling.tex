
% This is a sample document using the University of Minnesota, Morris, Computer Science
% Senior Seminar modification of the ACM sig-alternate style. Much of this content is taken
% directly from the ACM sample document illustrating the use of the sig-alternate class. Certain
% parts that we never use have been removed to simplify the example, and a few additional
% components have been added.

% See https://github.com/UMM-CSci/Senior_seminar_templates for more info and to make
% suggestions and corrections.

\documentclass{sig-alternate}
\usepackage{color}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\tiny\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%%%%% Uncomment the following line and comment out the previous one
%%%%% to remove all comments
%%%%% NOTE: comments still occupy a line even if invisible;
%%%%% Don't write them as a separate paragraph
%\newcommand{\mycomment}[1]{}

\begin{document}

% --- Author Metadata here ---
%%% REMEMBER TO CHANGE THE SEMESTER AND YEAR AS NEEDED
\conferenceinfo{UMM CSci Senior Seminar Conference, December 2017}{Morris, MN}

\title{Thread Scheduler Efficiency Improvements \\ for Multicore Systems [Draft]}

\numberofauthors{1}

\author{
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
\alignauthor
Daniel C. Frazier\\
	\affaddr{Division of Science and Mathematics}\\
	\affaddr{University of Minnesota, Morris}\\
	\affaddr{Morris, Minnesota, USA 56267}\\
	\email{frazi177@morris.umn.edu}
}
\maketitle

\begin{abstract}

Thread scheduling is a problem that has been around since the 1960s. By the early 2000s, thread scheduling was commonly believed to be solved in the Linux community. However, with the rising popularity of multiprocessor and multicore systems, and the rapidly developing requirements driven by new hardware, the problem space has become considerably more complex. This paper will describe some newly found issues in the Linux scheduler, their fixes, and two new schedulers designed for improved performance. Each of the developments meaningfully improve the average efficiency of popular relevant benchmarks.

\end{abstract}

\keywords{Scheduling; thread migration; multicore; multiprocessing; lock contention; last-level cache misses; efficiency}

\section{Introduction}
\label{sec:intro}

Using any modern computer, there is an expectation that the operating system is running at all times (largely in the background) and that multiple user programs and system programs should be able to run concurrently. Modern programs also often need to run more than one independent task at one time. A program can achieve this by employing \emph{threads}. Programs that involve long independent computations or programs with a graphical interface often benefit from employing threads.

For example, imagine a photo editing program that can apply an expensive filter operation. If the program wasn't multithreaded, the user interface and the filter operation would be executed in the same thread. When instructing the program to execute the filter on a large image, the user interface wouldn't be able to respond to any events (like clicking the mouse) until the filter operation was finished. To make the program more responsive, we can separate the user interface into its own thread and spawn new threads when the user initiates expensive operations.

There are many choices of hardware for a given computational problem. Processors used on most modern computing systems employ multiple cores. Some systems have multiple processors, each of which also have multiple cores. To take advantage of this hardware, threads should be distributed accross the cores wisely. In this paper, we will cover some important decisions and trade-offs that are made in order to allocate tasks to these resources to maximize performance.

\section{Background}
\label{sec:bg}

In this section, we will broadly establish how threading, caching, and scheduling works. These concepts will prepare us to establish causes of something called cache misses in the execution of programs using the current Linux thread scheduler. We will establish information necessary to understand some recent efforts made to improve thread scheduling on multiprocessor and multicore systems on Linux.

\subsection{Threads and Scheduling}
\label{sec:threads}

Threads are tied to the \emph{processes} that spawn them. A process always has at least one thread. Processes are typically independent of each other while threads exist within a process. \emph{Context switching} within a CPU is the process of storing and restoring the state of a process or thread so that execution can be paused or resumed. A process's state consists of resources that each of its threads should have access to, dominantly compiled code and data. A thread's state is used at runtime but is also stored to pause or resume a thread's execution.

The \emph{scheduler} is the part of the operating system that is responsible for managing and distributing the CPU runtime which each of these processes and their respective threads receive. New threads and processes are added to the scheduler when they are made.~\cite{Lozi:2016} The current implementation of the scheduler on Linux is hierarchical and consists of modules. The primary module is the \emph{scheduler core}, and it interfaces with modules called \emph{scheduler classes}. The \emph{Completely Fair Scheduler (CFS)} is the default scheduler class and is used for tasks that are not real-time critical. The core scheduler runs a tick function that runs $\sim1000$ times per second and calls the tick function of the appropriate scheduler class. For real-time tasks, the First-in first-out (FIFO) or Round-Robin (RR) scheduler classes are used but out of brevity we will not be covering these.~\cite{SchedThesis}

Before we discuss how the CFS schedules tasks, we should first understand how and where cache exists on most multicore systems, how a system can maintain cache coherence, how threads can work on the same data without conflicting with each other, and a few other related system performance concepts.

\subsection{Cache on NUMA Systems}
\label{sec:cache}

When a program is running, its memory is stored in the RAM. The RAM physically exists far away from the CPU relative to the ~\emph{cache}. Imagine a program that sums up an array of one million integers and they all fit into RAM. It would be very slow for the CPU to request from the RAM integers one at a time. Cache allows us to speed up this process by taking a chunk of data that the system predicts will be used frequently, and migrating it from the RAM into cache.

The cache that a system has is hardware dependent. In a \emph{non-uniform memory access (NUMA)} system, there are many levels of cache and they exist in a hierarchy. The defining property of NUMA systems is that different levels of cache have different access latencies.  Levels of cache that are physically closer to a core or processor are faster than levels that are farther. Cache can be integrated into, on top of, or outside of a core or processor. Lower levels of cache are smaller because of space restrictions. Most modern systems are multiprocessor multicore NUMA systems.

In a typical multiprocessor multicore setup, at the lowest level of the cache hierarchy is L1 cache which tends to exist integrated into or onto one or two cores. Then L2 connects some number of L1 caches together, repeating this pattern for however many levels of cache exist.


\begin{figure}
\centering
\psfig{file=CacheAbstract.pdf,width =3in}
\caption{Example cache setup for a dual-core processor system with three levels of cache}
\label{fig:cache}
\end{figure}


L1 cache is also called last \emph{level cache (LLC)}. Last level cache has the fastest data lookup times from a core because it is located nearest to the CPU. If data is not found in an L1 cache, it is called an \emph{LLC miss} and the data is searched for in the next level of cache. If the data can not be found in any level of cache, it is called a \emph{cache miss} and external memory is consulted for the data. Moving data on to the cache and closer to the CPU improves \emph{locality} because it makes the data more ``local'' to where it needs to be. Improving locality improves performance because the system works less hard to load the data it needs.

An important property of a system that employs cache is that it should be \emph{cache coherent}. When data is written into cache, it must propogate to all levels of cache accessible to the processor that issued the write. There are increasing read/write latencies for each level of cache away from a processor. Any changes to a portion of data in an L1 cache must propogate to L2, L3, etc. A system that is cache coherent is a system that satisfies the invariant that any data that is read that pertains to a certain portion of cached data must be consistent with the most recent write to that portion of data. This becomes a difficult problem in the instance that two different cores have both cached the same portion of data and are making concurrent reads and writes to this data.~\cite{Systems}

Context switching between two cores involves ensuring that the replacing thread has the resources it needs available on its new core's cache. Recall that a processor's state is much larger than thread's state. Because of this, context switching is typically fastest between threads who share a process because the processor's state wouldn't need to migrate to another cache.

Some processor setups involve sharing cache between processors. Cache sharing is not ideal, though, because a mechanism must be emplaced to arbitrate \emph{synchronous} access to shared data (synchronous access will be defined in the following subsection.) Further, per-proccessor cache can be physically integrated into processors in order to minimize cache access latency where shared cache can not. The most common approach to maintain strict consistency in cache is to somehow \emph{invalidate} cache entries. How cache invalidation works is out of the scope of this paper~\footnote{See Section 10.2.4 in Part II of Principles of Computer System Design by Saltzer, Jerome and Kaashoek, M. Frans for more information on cache invalidation.}.~\cite{Systems}

A NUMA system has NUMA nodes, but the definition of what a \emph{NUMA node} is changes depending on how NUMA is implemented on a machine. So let us consider how NUMA nodes were implemented for the system used in Lozi et al. On their system, L1 cache exists connecting four cores and NUMA nodes are L1 cache.

\subsection{Synchronicity and Locks}
\label{sec:locks}

When two threads have read and write access to the same portion of the memory, any read or write to that memory must be made synchronous to avoid race conditions. A race condition is a timing dependent error where two threads updating the same memory at the same time overwrite each other in a way that might cause the threads to function incorrectly. A system can be made synchronous by employing \emph{locks}. \textit{Principles of Computer System Design} by Saltzer and Kaashoek defines locks as ``A flag associated with a data object, set by a thread to warn concurrent threads that the object is in use and that it may be a mistake for other threads to read or write it.''~\cite{Systems}
When a thread needs to use an object which is associated with a lock, the thread should check the lock first before operating on the object. If an object's lock is already acquired, the thread should wait until the lock is released, then acquire it for itself.

An important problem faced by massively parallel programs is called lock contention. \emph{Lock contention} can be found in multithreaded programs where many threads frequently compete for access to the same lock. Threads of a program that are contending and which reside on different processors experience a higher than average LLC miss rate. This is a result of cache coherency. Because both of these threads are continually modifying the same data in separate cache, each change to one cache must propogate to the other before the other core reads the data.


\subsection{Completely Fair Scheduler (CFS)}
\label{sec:cfs}

The \emph{Completely Fair Scheduler (CFS)} was introduced in version 2.6.23 of the Linux kernel.~\cite{Jo:2017} We will discuss the CFS as presented in Lozi et al. The CFS is an implementation of the weighted fair queuing (WFQ) scheduling algorithm. The goal of the WFQ is to divide an arbitrary number of CPU cycles among threads, prioritizing more cycles for threads with larger weights.~\cite{Lozi:2016}

Threads that are running accumulate \emph{vruntime}, which is the runtime of a thread divided by its weight.~\cite{SchedThesis} Once a thread's vruntime is greater than the amount of runtime that the thread was scheduled for and there is another thread waiting to take this thread's place, the running thread is replaced by the next thread with the least vruntime. A thread may become preempted if another thread with a smaller vruntime awakens. Threads are organized in a priority queue called a \emph{runqueue} that sorts ascending on vruntime. Under normal conditions, the thread that replaces the current thread is the thread which needs most to run, which is the thread with the least vrtuntime, the first thread in the runqueue.~\cite{Lozi:2016}

In order for the scheduler to work on multiple cores, each core must have its own runqueue. If all cores shared one runqueue, each core would need to make frequent synchronous requests for process and thread state information from other cores~\cite{Lozi:2016}. For the scheduler to function properly and efficiently, it must keep each of the runqueues balanced. If runqueues are not balanced, then a core is left idle and needs to request work from another core to keep busy. External calls between cores are expensive and should be minimized. Most schedulers, including the CFS, run a load-balancing algorithm that tries its best to keep runqueues balanced. Load-balancing was simple for single-core systems, but with multi-core systems, bugs have found their way into the system and persist until at least Linux kernel version 4.3.

\begin{quote}
Our recent experience with the Linux scheduler revealed that the pressure to work around the challenging properties of modern hardware, such as non-uniform memory access latencies (NUMA), high costs of cache coherency and synchronization, ... resulted in a scheduler with an incredibly complex implementation. As a result, the very basic function of the scheduler, which is to make sure that runnable threads use idle cores, fell through the cracks.~\cite{Lozi:2016}
\end{quote}

\section{Methods}
\label{sec:methods}

So far we have established what threads are, how cache works and where it resides on NUMA systems, how the CFS scheduler works, how locks work and why they are important. Now, we will discuss four bugs Lozi et al. found in the CFS load-balancer and their fixes, which substantially improved scheduler efficiency. Later, we will show two more solutions that improve efficiency for certain kinds of programs running on multiprocessor multicore NUMA systems.

\subsection{Load-Balancing the CFS}
\label{sec:loadbalance}

As we mentioned in Section \ref{sec:cfs}, modifications were made to the CFS that introduced bugs that caused processors to remain idle even when there were threads available. Work by Lozi et al. has identified four bugs that were responsible for this behaviour. These bugs have remained hidden because, while they corrode performance, they are not obvious. They do not make programs freeze or crash and their effects only last a few hundred milliseconds at a time, which is too short for common performance tools like \textbf{top} to detect. Lozi et al. designed new tools that observe the Linux scheduler more closely. Using these tools helped these researches locate the problems.~\cite{Lozi:2016}

In order to understand these bug fixes, as described in Lozi et al., we must explain a simplified version of the CFS load balancer.~\cite{Lozi:2016}

\subsubsection{Load Metric}
\label{sec:loadmetric}

The load-balancing algorithm tracks a metric called \emph{load} to best distribute threads to cores. Defining what load should be is tricky. Balancing load such that each core has the same number of threads is not ideal because threads have priorities, and if all of the high-priority threads happened to be placed on one core while all low-priority threads were placed on another, the low-priority threads would be receiving much more runtime than they should be in relation to the high-priority threads. Balancing load such that each core has roughly the same amount of weight is not ideal either, because, if there was one thread that was nine times more important than nine low-priority threads, that important thread would be left on a core all alone. That seems acceptable, but consider the case that this high-priority thread sleeps frequently. Its core would be left idle for an unnacceptable amount of time. The idle core would need to ask other cores for more work to keep busy in the downtime, which is an expensive operation for both cores involved.~\cite{Lozi:2016}

The current implementation of CFS uses a combination of a thread's weights and average CPU use divided by the number of all threads in the parent process for the load metric. It divides by the number of all threads in the parent process in order to remain fairness so that two processes that have different numbers of threads of the same priority still get equal runtime.~\cite{Lozi:2016}

\subsubsection{Load-Balancing Algorithm}
\label{sec:loadbalancealg}

\begin{figure}
\centering
\psfig{file=NUMA.pdf,width =3in}
\caption{32-core machine with four NUMA nodes. It only takes two hops to get from any core to another node. The shades represent scheduling domains relative to the first core. From Lozi et al.~\cite{Lozi:2016}}
\label{fig:domains}
\end{figure}


For load balancing, cores exist in a hierarchy where each level is called a \emph{scheduling domain}. The groups within each level are based on how cores share resources within the machine. The lowest level of scheduling domain is a single core. In the machine described in Section~\ref{sec:cache} there were 32 cores. These cores represented the first level of scheduling domains. The second domains were determined by groups of eight adjacent processors which made four NUMA nodes. The third level was formed by groupings of nodes that are within one hop of each other. The final level was all of the nodes as one unit. See Figure~\ref{fig:domains}.~\cite{Lozi:2016}


{\SetAlgoNoLine%
	\DontPrintSemicolon 
	\SetKwFunction{Frog}{Function running on each cpu $\textit{cur\_cpu:}$}
\begin{algorithm}
% \Indm\Indmm
% \caption{Function running on each cpu $\textit{cur\_cpu:}$}
%\begin{minipage}{.92\linewidth}
%	Function running on each cpu $\textit{cur\_cpu:}$
%	Lots more text running on and on and on\ldots 
%	Lots more text running on and on and on\ldots 
%	Lots more text running on and on and on\ldots 
%	Lots more text running on and on and on\ldots 
%	Lots more text running on and on and on\ldots 
%	Lots more text running on and on and on\ldots 
%	Lots more text running on and on and on\ldots 
%\end{minipage}
%\KwFunction{Function running on each cpu $\textit{cur\_cpu:}$}
% \Indp\Indpp
%    \BlankLine
\Frog\;
	\ForAll{sd in sched\_domains of cur\_cpu}{
		\eIf{sd has idle cores}{first\_cpu = $1^{st}$ idle CPU of sd}
		{
			first\_cpu = $1^{st}$ CPU of sd 
		}
		\If{cur\_cpu $\neq$ first\_cpu}{
			$\textbf{continue}$
		}
		\ForAll{sched\_group sg $\textbf{in}$ sd}{
			sg.load = average loads of CPUs in sg
		}
		busiest = overloaded sg with the highest load\;
			\quad($\textbf{or}$, if inexistent) imbalanced sg with highest load\;
			\quad($\textbf{or}$, if inexistent) sg with the highest load\;
		local = sg containing cur\_cpu \;
		\If{busiest.load $\leq$ local.load}{
			$\textbf{continue}$
		}
		busiest\_cpu = pick busiest cpu of sg\;
		try to balance load between busiest\_cpu and cur\_cpu\;
		\If{load cannot be balanced due to tasksets}{
		exclude busiest\_cpu, $\textbf{goto}$ line 18
		}
	}
\label{fig:cfsloadbalancer}	
\caption{Simplified CFS Load Balance algorithm from Lozi et al.~\cite{Lozi:2016} A separate instance of this function is running on each CPU, $\textit{cur\_cpu:}$ CPUs are cores.}
\end{algorithm}
}


A naiive approach to load balancing would be to compare load on each core and transfer tasks from cores with the highest load to cores with the lowest load. This approach would not put into consideration improving thread locality or NUMA. Instead, the CFS load-balancing algorithm is executed on each CPU for each scheduling domain that the CPU is a part of starting from the first level to last levels of scheduling domain. A simplification of the CFS load balancing algorithm can be found in Figure \ref{fig:cfsloadbalancer}.

In the CFS load balancer, one core per scheduling domain is responsible for performing the load-balancing for that domain. That core is either the first core that is idle or the first core of the scheduling domain (Lines 2-9). Then for each scheduling group within the scheduling domain, the average load is computed (Lines 10-12) and the group with the highest load is determined to be the busiest group (Line 13). If the busiest group is less busy than the group containing this CPU (local), then the load is considered balanced for this level and continues on to consider the next scheduling domain (Lines 14-17). At lines 18 to 23 the current core balances load between the bustiest core and itself.~\cite{Lozi:2016}

Several optmizations made the scheduler such that it runs the load balancing operation less often. Lines 2-8 is an example of this where if any of the cores are idle, the first idle core in the scheduling domain is chosen to consider load balancing. If all cores are busy, the first core of the scheduling domain is chosen. There is also a power optimization where idle cores don't run the load balancing algorithm until awoken by an overloaded core. To improve cache locality, when an idle thread is awoken by an active thread, the scheduler prefers to assign the awoken thread to the same core so that they share a last-level cache.~\cite{Lozi:2016}

\subsection{Bugs and Fixes for the CFS}
\label{sec:cfsbugs}

Now that we know how the load balancer functions, we will now dive into the bugs that occur as a result of the complex, strict requirements that have built up over time on the CFS. Performance results from Lozi et al. were gathered using the \emph{NAS Parallel Benchmark (NPB)}. The NPB is a set of parallel programs that are executed and monitored to evaluate the efficiency of massively parallel computing systems (supercomputers).

\subsubsection{The Group Imbalance bug}
\label{sec:cfsfault_grpimbalance}

Lozi et al. found that the scheduler was periodically not balancing load due to two reasons. The hierarchical design and complexity of the load metric. The researchers were running a 64 threaded  Process A on node A simultaneously with a single-threaded Process B on node B. Because of group schedule feature in the definition of load metric where the load of a thread is divided by the number of threads in the parent process, the amount of load for each of A's threads are 1/64 of the load of the one B thread. On their system they observed that two nodes were underloaded and should have been stealing work from more loaded cores on other nodes, but this did not happen. This is due to the hierarchical design of the load balancer. When the load balancer considers stealing threads, it does not consider the load of a single core but rather the load of the whole scheduling group. In this case, the node with the B process had the same load as the node with the A process, so node B did not try to steal work from node A even though there were cores idle and available for work.~\cite{Lozi:2016}

They fixed this problem by instead of defining load of a scheduling group as the average load of cores in the group, it is defined by the load of the core with the minimum load in the scheduling group. By fixing this issue, the efficiency of the single-threaded process B remained the same, however the completion time of process A decreased by 13 percent. On a certain parallel program within the NPB called lu, this bug fix improved performance by 13 times because the bug compounded lock contention by colocated threads.~\cite{Lozi:2016}

\subsubsection{The Scheduling Group Construction bug}
\label{sec:cfsfault_grpconstruct}

There is a function in Linux called taskset which allows an application to pin its threads to certain available cores. On certain machines, sequential NUMA nodes may be located more than one hop from each other. When Linux spawns new thread, they are created on the same node as the parent thread. The parent thread only exists on one core, so all spawned threads are made on the same node. The first scheduling group is constructed by all adjacent nodes to one node (say, Node 0), and following scheduling groups are constructed by adjacent nodes to nodes that were not in any of the previous groups. On a system it is possible for two nodes to be within one hop of the starting nodes of each group, but be two hops from each other. See Figure \ref{fig:cfs_schedgroups}.~\cite{Lozi:2016}

Within that system, the two scheduling groups constructed are {0,1,2,4,6} and {1,2,3,4,5,7}. Nodes 1 and 2 are two hops from each other but occur in both scheduling groups. When load balancing runs on Node 2 and it should steal work from an overloaded Node 1, it will look between the scheduling groups to compare which is lower. But both scheduling groups contain Node 1 and 2, so both scheduling groups' average load will be the same! Node 2 will never steal work.~\cite{Lozi:2016}

They fixed this bug by constructing scheduling groups relative to each core's own perspective. This allows nodes that are two hops away that should otherwise be able to share work to be a part of different scheduling groups and successfully contend for load balancing. Fixing this bug improved efficiency of 9 parallel programs in the NPB problems mostly by around 2 times, but for one problem, lu, up to 27 times. lu is an extreme example where all threads were being allocated only on one core.~\cite{Lozi:2016}

\begin{figure}
\centering
\psfig{file=CFS_AdjacentNodes.png,width =3in}
\caption{8-node AMD Bulldozer machine from Lozi et al.~\cite{Lozi:2016}}
\label{fig:cfs_schedgroups}
\end{figure}

\subsubsection{The Overload-on-Wakeup bug}
\label{sec:cfsfault_overload}


There exists an optimization in thread wake-up code where threads that are awakened by other threads are placed on the same core for improved cache locality. This becomes a problem when the core with the requesting thread is already overloaded. The thread will join the runqueue on this busy core rather than consider being migrated to an idle core elsewhere. For some workloads this is acceptable, but simply making the most of cache isn't always the best decision. This bug occurs primarily on database systems. The following sequence of events will illustrate how the Overload-on-Wakeup bug occurs.~\cite{Lozi:2016}

Consider two nodes running on a database system. Node A and Node B both have a database thread. Assume a temporary thread is created by the kernel on Node A for some background function such as system logging. During this time, the load balancer detects that the node is overloaded due to the new thread, and decides to migrate some thread to Node B. If it decides to move the temporary thread, that will pose no issue for performance. If it migrates the database thread, two database threads which sleep and awaken frequently will be running on the same node. The scheduler does not differentiate between cores that are idle for a short time versus a long time, so when the scheduler considers thread migration destinations, it might see a Node, which happens to have a database thread that is currently idle, and decide that the node needs more work.~\cite{Lozi:2016}

Lozi et al. fixed this bug by modifying the thread wake up code. The scheduler first checks to see if the core that the thread was already assigned to is idle. If it is, then wake up the thread on that core. If the power manager policy of the system is set up so that cores never enter low-power mode and there are idle cores, the scheduler chooses the core that has been idle for the most time and assigns the thread to that core. This fix is only relevant to workloads where threads frequently sleep. Their fix improves performance by 13.2\% on a full TPC-H database workload but up to 22.2\% for certain queries.~\cite{Lozi:2016}

\subsubsection{The Missing Scheduling Domains bug}
\label{sec:cfsfault_missingsched}

This bug is actually something that was already fixed but regressed on Linux kernel version 3.19 (to at least 4.3) when an important line of code was removed in a refactor that causes the system to misrepresent the number of scheduling domains that are available for assigning threads to. This ended up causing load-balancing to never happen on all nodes, meaning processes, subprocesses and their threads to stop looking for other NUMA nodes to assign themselves to. Nodes that do not have any threads will never receive threads and nodes that do have threads will accumulate all of the spawned threads.

This bug required that one of the cores become disabled and re-enabled. So while rare, reintroducing the removed line of code increased efficiency in a certain tested program by a maximum of 138 times.~\cite{Lozi:2016}

\subsection{Shuffler}
\label{sec:shuffler}

Research by Kumar, Rajiv, Laxmi and Bhuyan studied 20 certain parallel algorithms and found that with many of them, adding more threads began to degrade program performance rather than improve. The researchers monitored time spent acquiring locks and number of LLC misses experienced while running these programs. They identified the source of this problem to be \emph{lock contention}.

As mentioned in the concepts section, lock contention can be found in multithreaded programs where many threads repeatedly compete for access to the same lock. The costs in this involve the transfer of the lock and the propogation of cache associated with the lock. The problem is further compounded if the threads are not located within the same processor. If the threads of that multithreaded program were prioritized to be placed on one processor, then the program would experience a lower LLC miss rate. Neither the CFS nor the Solaris scheduler differentiate between threads of a single-threaded program versus the threads of a multithreaded program. This prevents the scheduler from using that metadata in its thread distribution mechanism. The following thread scheduler named \emph{Shuffler} by Kumar et al. takes this into account. The Shuffling framework was designed for multiprocessor multicore NUMA systems, and implemented on a 64-core 4-processor machine running Oracle Solaris 11.~\cite{Kumar:2014}

The Shuffling approach is for the scheduling algorithm to take into account what threads are contending for locks on what processors and migrate \textbf{whole threads} (rather than lock and cache) such that they share processors. Threads that are reported to have higher lock acquisition times are threads that are requesting locks from outside of their processor. These are the threads that should be migrated to share processors. Details on the shuffling framework are as follows.~\cite{Kumar:2014}

\begin{description}
\item [Monitor Threads] \ \\
First, we monitor the amount of time user threads spend acquiring locks and store it in a data structure. 

\item [Form Thread Groups] \ \\
If sampled lock times exceed a certain threshold, threads are sorted by their lock acquisition times and grouped by their order in the sorted data structure. There are as many groups as processors that will be used to run the program. This procedure is run every 200 ms to continue to form groups of threads that should share processors.

\item [Perform Shuffling] \ \\
On an iteration of the shuffling procedure (every 500 ms), shuffling checks to make sure that if any threads aren't on processors that they were grouped to, they are migrated. Threads that are already on the processor that they were assigned to do not migrate. If the way that threads interacted with eachother doesn't change, the shuffling step is effectively skipped.~\cite{Kumar:2014}
\end{description}


Now that we know how the Shuffling Framework works, let's review the results that implementing it gives us. Kumar et al. chose certain programs from several benchmarks that contained high lock contention to assess Shuffler performance: SPEC jbb2005 (JB), PBZIP2 (PB), and programs from three suites PARSEC(BT, FA, FS, SC), SPEC OMP2001 (AL, AM, AS, EQ, FM, GA, GL, MG, SM, WW) and SPLASH2(OC, RT, RX, VL). Figure \ref{fig:shuf_performance} details the improvements that Shuffling made relative to the default Solaris scheduler and three other schedulers. A positive number denotes that that scheduler was that more efficient than the default Solaris scheduler for that program. Each of the programs are multithreaded. The one they highlight the most was the Body Tracking (BT) algorithm of whose performance improved by 54 percent.~\cite{Kumar:2014} Figure \ref{fig:shuf_vs_solaris} compares the efficiency of Shuffler versus Solaris for the 20 same multithreaded programs. With these results we can say that multithreaded programs that had high lock contention perform faster under Shuffler than Solaris, but say nothing about the performance difference of programs who don't experience high lock contention.
\begin{figure}
\centering
\psfig{file=Performance_Shuffler.png,width =3in}
\caption{Shuffler performance relative to Solaris across 20 high-lock time multithreaded programs~\cite{Kumar:2014}}
\label{fig:shuf_performance}
\end{figure}

\begin{figure}
\centering
\psfig{file=SolarisVsShuffling.png,width =3in}
\caption{Lock arrival times ranges with Solaris vs. Shuffling. From Kumar et al.~\cite{Kumar:2014}}
\label{fig:shuf_vs_solaris}
\end{figure}

We just saw that lock contention is a problem for multithreaded parallel programs. Next we will cover a scheduler that takes a different approach to alleviating the lock contention problem. FLSCHED is a scheduler whose implementation has no locks, reduces context switches, and makes more efficient scheduling decisions than CFS.~\cite{Jo:2017}

\subsection{Xeon Phi Manycore Coprocessor}
\label{sec:flsched}

FLSCHED was built for maximal efficiency on manycore processors. Manycore processors contain upwards of about 20 cores. They continue to become more powerful and as such, more popular. The Xeon Phi is a family of manycore coprocessors that allow a primary processor to offload expensive work. The latest version of Xeon Phi as of September 2 2017 had up to 76 cores. The number of cores a processor has is expected to increase given the popularity and importance of highly-parallel algorithms such as machine learning. The CFS was not built for this degree of parallelism. The cost of context switching continues to increase as hardware becomes capable of solving larger problems. Unfortunately, the density of cores on manycore processors causes them to have very small L1 cache. As the number of cores increase, the negative impact of lock contention on scheduler performance increases exponentially.

The context switch overhead for the Xeon Phi is much larger than usual processors because it contains additional vector-processing register sets.~\cite{Jo:2017} A machine that supports vector processing allows one to perform instructions on vectors of data rather than scalars~\cite{Mellon}. The Xeon Phi processor used by FLSCHED researchers Jo et al. had 57 cores.~\cite{Jo:2017}

\subsubsection{Lockless Thread Scheduler}
\label{sec:flsched_about}

The design goals and requirements for the CFS differ from the FLSCHED. The CFS was designed with responsiveness, fairness and load-balancing in mind because it was intended for desktop and server use. The FLSCHED was designed strictly with efficiency and computational throughput in mind. In the CFS, a great deal of state information is considered to decide what threads run and when. Since FLSCHED was intended for manycore parallel processors, it is more impactful to simply make decisions faster rather than more purposefully.~\cite{Jo:2017}

The CFS implementation employs 11 locks which are used for load balancing mechanisms, runqueue management, runtime statistics updates, and additional scheduler features. FLSCHED itself has no locks. It manages this by removing runtime statistics entirely from the scheduler, as it does not depend on them to make decisions. FLSCHED does not use periodic load balancing, so it does not provide the feature to limit maximum CPU bandwidth nor any of the CFS group features. While the FLSCHED substitutes for the CFS, it still runs through the scheduler core which employs locks. Contexts switches are requested, rather than performed outright. Commitments to context switch requests are delayed on purpose to minimize the number of context switches.~\cite{Jo:2017}

Timeslices in FLSCHED are assigned Round-Robin. The only managed scheduling information is the timeslices that threads receive. Thread preemption in FLSCHED mostly occurs because there is another thread with a higher priority. Rather than immediately preempting the running thread to replace with the new thread, FLSCHED reorders runqueues such that the important thread will come next after a normal task switch.~\cite{Jo:2017}


\subsubsection{FLSCHED Perfomance}
\label{sec:flsched_performance}

 To evaluate FLSCHED performance, Jo et al. used the NPB. NPB version 3.3.1 consists of 10 problems, but two of which can not run on the Xeon Phi due to memory constraints. The tests were ran on FLSCHED, the CFS, and two more basic schedulers called First-in first-out (FIFO) and Round Robin (RR) in order to compare. From the results of these tests (Figure \ref{fig:flsched_npb}), you can see that the FLSCHED scales better as the number of threads increases for six out of eight problems. Efficiency of the ep and is programs degraded under FLSCHED.~\cite{Jo:2017}

\begin{figure*}
\centering
\psfig{file=FLSCHED_NPBCMP.png,width =5.1in}
\caption{FLSCHED performance comparison of various schedulers on programs in the NAS Parallel Benchmark. From Jo et al.~\cite{Jo:2017}}
\label{fig:flsched_npb}
\end{figure*}

The researchers traced the cause of the efficiency improvements to minimizing a certain kind of lock called a spin lock from their scheduler. A spinlock is a type of lock that loops indefinitely checking whether it has unlocked yet. These locks are used if the lock is expected to take a short amount of time to avoid giving up the core to another context. Figure \ref{fig:flsched_spinlock} shows the percent of time that each of these schedulers spend processing spin locks. Time spent on spin locks was more than halved for programs cg, mg, sp and ua.~\cite{Jo:2017}

\begin{figure}
\centering
\psfig{file=FLSCHED_SpinLock.png,width =3in}
\caption{ Percent of time that various schedulers spend executing a certain type of lock called a spin lock. Lock times were measured throughout the runtime of the NAS Parallel Benchmark at 1,600 threads. From Jo et al.~\cite{Jo:2017}}
\label{fig:flsched_spinlock}
\todo[inline]{Should be a "table" not a figure}
\end{figure}

Finally, Jo et al. also measured per-function diagnostics of both the CFS and FLSCHED using ftrace. The functions measured were crucial scheduler tasks. The CFS was compared to FLSCHED by running hackbench, a well known scheduler stress test, and measuring number of function calls and time spent executing them. See Figure \ref{fig:flsched_func}.\cite{Jo:2017}

\begin{figure*}
\centering
\psfig{file=FLSCHED_SchedulerCMP.png,width =5.3in}
\caption{Average time and function call counts for critical scheduler tasks. The numbers were gathered using ftrace while running the hackbench stress test for thread schedulers. From Jo et al.~\cite{Jo:2017}}
\label{fig:flsched_func}
\todo[inline]{Should be a "table" not a figure}
\end{figure*}

\section{Conclusions}
\label{sec:conclusions}

Most of the modifications to the CFS in Section \ref{sec:cfsbugs} improves user experience and system efficiency for Linux systems that use the CFS. The CFS functions well for the user and server systems it was designed to support, but falls short when it comes to highly parallel programs. The Shuffler makes highly parallel programs running on multicore multiprocessor systems function more efficienctly than on the CFS by migrating threads to make better use of hardware. The FLSCHED makes manycore parallel machines intended for problem-solving more efficient by removing features that a desktop or server system would normally have and implementing a simple, greedy approach to program execution.

\section*{Acknowledgments}
\label{sec:acknowledgments}
%Thanks to Nic McPhee, Dan Stelljas and Cleo for their help in

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{scheduling}  
% Remember to run:
% latex bibtex latex latex
% to resolve all references

\end{document}
